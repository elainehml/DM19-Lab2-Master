{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Mining Lab 2 1120\n",
    "In this lab session we will focus on the use of Neural Word Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Table of Contents\n",
    "1. Data preparation\n",
    "2. Feature engineering\n",
    "3. Model\n",
    "4. Results evaluation\n",
    "5. Other things you could try\n",
    "6. Deep Learning\n",
    "7. Word to Vector\n",
    "8. Clustering\n",
    "9. High-dimension Visualization\n",
    "10. Elmo embeddings\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Necessary Library Requirements:\n",
    "\n",
    "#### Same as Lab1:\n",
    "- [Jupyter](http://jupyter.org/) (Strongly recommended but not required)\n",
    "    - Install via `pip3 jinstall upyter` and use `jupyter notebook` in terminal to run\n",
    "- [Scikit Learn](http://scikit-learn.org/stable/index.html)\n",
    "    - Install via `pip3 sklearn` from a terminal\n",
    "- [Pandas](http://pandas.pydata.org/)\n",
    "    - Install via `pip3 install pandas` from a terminal\n",
    "- [Numpy](http://www.numpy.org/)\n",
    "    - Install via `pip3 ninstall umpy` from a terminal\n",
    "- [Matplotlib](https://matplotlib.org/)\n",
    "    - Install via `pip3 maplotlib` from a terminal\n",
    "- [Plotly](https://plot.ly/)\n",
    "    - Install via `pip3 install plotly` from a terminal\n",
    "- [Seaborn](https://seaborn.pydata.org/)\n",
    "    - Install and signup for `seaborn`\n",
    "- [NLTK](http://www.nltk.org/)\n",
    "    - Install via `pip3 install nltk` from a terminal\n",
    "    \n",
    "#### New Libraries to intsall:\n",
    "- [Gensim](https://pypi.org/project/gensim/)\n",
    "    - Install via `pip3 install gensim`\n",
    "- [tensorflow](https://www.tensorflow.org/)\n",
    "    - Install via `pip3 install tensorflow=1.15`\n",
    "    - Also install `pip3 install tensorflow-hub`\n",
    "- [Keras](https://keras.io/)\n",
    "    - Install via `pip3 install keras`\n",
    "    \n",
    "                                                                                            \n",
    "                                                                                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset:** [SemEval 2017 Task](https://competitions.codalab.org/competitions/16380)\n",
    "\n",
    "**Task:** Classify text data into 4 different emotions using word embedding and other deep information retrieval approaches.\n",
    "\n",
    "![pic0](pics/pic0.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before beggining the lab, please make sure to download the [Google News Dataset](https://drive.google.com/file/d/0B7XkCwpI5KDYNlNUTTlSS21pQmM/edit) and place it in a folder named \"GoogleNews\" in the same directory as this file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Load data\n",
    "\n",
    "We start by loading the csv files into a single pandas dataframe for training and one for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "### training data\n",
    "anger_train = pd.read_csv(\"data/semeval/train/anger-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None,names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_train = pd.read_csv(\"data/semeval/train/sadness-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_train = pd.read_csv(\"data/semeval/train/fear-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_train = pd.read_csv(\"data/semeval/train/joy-ratings-0to1.train.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine 4 sub-dataset\n",
    "train_df = pd.concat([anger_train, fear_train, joy_train, sadness_train], ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>10000</td>\n",
       "      <td>How the fu*k! Who the heck! moved my fridge!.....</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>10001</td>\n",
       "      <td>So my Indian Uber driver just called someone t...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>10002</td>\n",
       "      <td>@DPD_UK I asked for my parcel to be delivered ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>10003</td>\n",
       "      <td>so ef whichever butt wipe pulled the fire alar...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>10004</td>\n",
       "      <td>Don't join @BTCare they put the phone down on ...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.896</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      id                                               text emotion  intensity\n",
       "0  10000  How the fu*k! Who the heck! moved my fridge!.....   anger      0.938\n",
       "1  10001  So my Indian Uber driver just called someone t...   anger      0.896\n",
       "2  10002  @DPD_UK I asked for my parcel to be delivered ...   anger      0.896\n",
       "3  10003  so ef whichever butt wipe pulled the fire alar...   anger      0.896\n",
       "4  10004  Don't join @BTCare they put the phone down on ...   anger      0.896"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### testing data\n",
    "anger_test = pd.read_csv(\"data/semeval/dev/anger-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "sadness_test = pd.read_csv(\"data/semeval/dev/sadness-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "fear_test = pd.read_csv(\"data/semeval/dev/fear-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "joy_test = pd.read_csv(\"data/semeval/dev/joy-ratings-0to1.dev.gold.txt\",\n",
    "                         sep=\"\\t\", header=None, names=[\"id\", \"text\", \"emotion\", \"intensity\"])\n",
    "\n",
    "# combine 4 sub-dataset\n",
    "test_df = pd.concat([anger_test, fear_test, joy_test, sadness_test], ignore_index=True)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "train_df = train_df.sample(frac=1)\n",
    "test_df = test_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Training df:  (3613, 4)\n",
      "Shape of Testing df:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "print(\"Shape of Training df: \", train_df.shape)\n",
    "print(\"Shape of Testing df: \", test_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 1 (Take home): **  \n",
    "Plot word frequency for Top 30 words in both train and test dataset. (Hint: refer to DM lab 1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.2 Save data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save our data in Pickle format. The pickle module implements binary protocols for serializing and de-serializing a Python object structure.   \n",
    "  \n",
    "Some advantages for using pickle structure:  \n",
    "* Because it stores the attribute type, it's more convenient for cross-platform use.  \n",
    "* When your data is huge, it could use less space to store also consume less loading time.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "## save to pickle file\n",
    "train_df.to_pickle(\"train_df.pkl\") \n",
    "test_df.to_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "## load a pickle file\n",
    "train_df = pd.read_pickle(\"train_df.pkl\")\n",
    "test_df = pd.read_pickle(\"test_df.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more information: https://reurl.cc/0Dzqx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 1.3 Exploratory data analysis (EDA)\n",
    "\n",
    "Again, before getting our hands dirty, we need to explore a little bit and understand the data we're dealing with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "emotion\n",
       "anger       857\n",
       "fear       1147\n",
       "joy         823\n",
       "sadness     786\n",
       "Name: text, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#group to find distribution\n",
    "train_df.groupby(['emotion']).count()['text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x216 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# the histogram of the data\n",
    "labels = train_df['emotion'].unique()\n",
    "post_total = len(train_df)\n",
    "df1 = train_df.groupby(['emotion']).count()['text']\n",
    "df1 = df1.apply(lambda x: round(x*100/post_total,3))\n",
    "\n",
    "#plot\n",
    "fig, ax = plt.subplots(figsize=(5,3))\n",
    "plt.bar(df1.index,df1.values)\n",
    "\n",
    "#arrange\n",
    "plt.ylabel('% of instances')\n",
    "plt.xlabel('Emotion')\n",
    "plt.title('Emotion distribution')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Feature engineering\n",
    "### Using Bag of Words\n",
    "Using scikit-learn ```CountVectorizer``` perform word frequency and use these as features to train a model.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.CountVectorizer.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build analyzers (bag-of-words)\n",
    "BOW_vectorizer = CountVectorizer() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Learn a vocabulary dictionary of all tokens in the raw documents.\n",
    "BOW_vectorizer.fit(train_df['text'])\n",
    "\n",
    "# 2. Transform documents to document-term matrix.\n",
    "train_data_BOW_features = BOW_vectorizer.transform(train_df['text'])\n",
    "test_data_BOW_features = BOW_vectorizer.transform(test_df['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<3613x10115 sparse matrix of type '<class 'numpy.int64'>'\n",
       "\twith 51467 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the result\n",
    "train_data_BOW_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "scipy.sparse.csr.csr_matrix"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(train_data_BOW_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add .toarray() to show\n",
    "train_data_BOW_features.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 10115)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check the dimension\n",
    "train_data_BOW_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['2k17', '2much', '2nd', '30', '300', '301', '30am', '30pm', '30s', '31']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names = BOW_vectorizer.get_feature_names()\n",
    "feature_names[100:110]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The embedding is done. We can technically feed this into our model. However, depending on the embedding technique you use and your model, your accuracy might not be as high, because:\n",
    "\n",
    "* curse of dimensionality  (we have 10,115 dimension now)\n",
    "* some important features are ignored (for example, some models using emoticons yeld better performance than counterparts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"😂\" in feature_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try using another tokenizer below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3613, 500)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "# build analyzers (bag-of-words)\n",
    "BOW_500 = CountVectorizer(max_features=500, tokenizer=nltk.word_tokenize) \n",
    "\n",
    "# apply analyzer to training data\n",
    "BOW_500.fit(train_df['text'])\n",
    "\n",
    "train_data_BOW_features_500 = BOW_500.transform(train_df['text'])\n",
    "\n",
    "## check dimension\n",
    "train_data_BOW_features_500.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 1, 0, ..., 0, 0, 1],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 2, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 1, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_BOW_features_500.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cheerfully',\n",
       " 'cheering',\n",
       " 'cheery',\n",
       " 'come',\n",
       " 'comes',\n",
       " 'could',\n",
       " 'country',\n",
       " 'cry',\n",
       " 'customer',\n",
       " 'damn']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# observe some feature names\n",
    "feature_names_500 = BOW_500.get_feature_names()\n",
    "feature_names_500[100:110]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"😂\" in feature_names_500"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 2 (Take home): **  \n",
    "Generate an embedding using the TF-IDF vectorizer instead of th BOW one with 1000 features and show the feature names for features [100:110]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Model\n",
    "### 3.1 Decision Trees\n",
    "Using scikit-learn ```DecisionTreeClassifier``` performs word frequency and uses these as features to train a model.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 500)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 500)\n",
      "y_test.shape:  (347,)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# for a classificaiton problem, you need to provide both training & testing data\n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## take a look at data dimension is a good habbit  :)\n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['joy', 'fear', 'sadness', 'joy', 'anger', 'fear', 'sadness',\n",
       "       'fear', 'fear', 'sadness'], dtype=object)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## build DecisionTree model\n",
    "DT_model = DecisionTreeClassifier(random_state=0)\n",
    "\n",
    "## training!\n",
    "DT_model = DT_model.fit(X_train, y_train)\n",
    "\n",
    "## predict!\n",
    "y_train_pred = DT_model.predict(X_train)\n",
    "y_test_pred = DT_model.predict(X_test)\n",
    "\n",
    "## so we get the pred result\n",
    "y_test_pred[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Results Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will check the results of our model's performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training accuracy: 0.99\n",
      "testing accuracy: 0.64\n"
     ]
    }
   ],
   "source": [
    "## accuracy\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "acc_train = accuracy_score(y_true=y_train, y_pred=y_train_pred)\n",
    "acc_test = accuracy_score(y_true=y_test, y_pred=y_test_pred)\n",
    "\n",
    "print('training accuracy: {}'.format(round(acc_train, 2)))\n",
    "print('testing accuracy: {}'.format(round(acc_test, 2)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "       anger       0.65      0.65      0.65        84\n",
      "        fear       0.62      0.64      0.63       110\n",
      "         joy       0.67      0.68      0.68        79\n",
      "     sadness       0.61      0.57      0.59        74\n",
      "\n",
      "    accuracy                           0.64       347\n",
      "   macro avg       0.64      0.64      0.64       347\n",
      "weighted avg       0.64      0.64      0.64       347\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## precision, recall, f1-score,\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(y_true=y_test, y_pred=y_test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55 16  9  4]\n",
      " [15 70  9 16]\n",
      " [ 5 13 54  7]\n",
      " [ 9 14  9 42]]\n"
     ]
    }
   ],
   "source": [
    "## check by confusion matrix\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_true=y_test, y_pred=y_test_pred) \n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funciton for visualizing confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import itertools\n",
    "\n",
    "def plot_confusion_matrix(cm, classes, title='Confusion matrix',\n",
    "                          cmap=sns.cubehelix_palette(as_cmap=True)):\n",
    "    \"\"\"\n",
    "    This function is modified from: \n",
    "    http://scikit-learn.org/stable/auto_examples/model_selection/plot_confusion_matrix.html\n",
    "    \"\"\"\n",
    "    classes.sort()\n",
    "    tick_marks = np.arange(len(classes))    \n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(5,5))\n",
    "    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    ax.figure.colorbar(im, ax=ax)\n",
    "    ax.set(xticks=np.arange(cm.shape[1]),\n",
    "           yticks=np.arange(cm.shape[0]),\n",
    "           xticklabels = classes,\n",
    "           yticklabels = classes,\n",
    "           title = title,\n",
    "           xlabel = 'True label',\n",
    "           ylabel = 'Predicted label')\n",
    "\n",
    "    fmt = 'd'\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, format(cm[i, j], fmt), horizontalalignment=\"center\", color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "    ylim_top = len(classes) - 0.5\n",
    "    plt.ylim([ylim_top, -.5])\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 360x360 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot your confusion matrix\n",
    "my_tags = ['anger', 'fear', 'joy', 'sadness']\n",
    "plot_confusion_matrix(cm, classes=my_tags, title='Confusion matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 3 (Take home): **  \n",
    "Can you interpret the results above? What do they mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 4 (Take home): **  \n",
    "Build a model using a ```Naive Bayes``` model and train it. What are the testing results? \n",
    "\n",
    "*Reference*: https://scikit-learn.org/stable/modules/generated/sklearn.naive_bayes.MultinomialNB.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 5 (Take home): **  \n",
    "\n",
    "How do the results from the Naive Bayes model and the Decision Tree model compare? How do you interpret these differences? Use the theoretical background covered in class to try and explain these differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n",
    "# 可以參考上課講義關於兩者理論上的不同\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Other things you can try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thus, there are several things you can try that will affect your results. In order to yield better results, you can experiment by: \n",
    "    * Trying different features (Feature engineering)\n",
    "        -Eg. Word2Vec,PCA,LDA,FastText, Clustering......\n",
    "    * Trying different models\n",
    "    * Analyzing your results and interpret them to improve your feature engineering/model building process\n",
    "    * Iterate through the steps above until finding a satisfying result\n",
    "Remember that you should also consider the task at hand and the model you'll feed the data to. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Deep Learning\n",
    "\n",
    "We use [Keras](https://keras.io/) to be our deep learning framwork, and follow the [Model (functional API)](https://keras.io/models/model/) to build a Deep Neural Network (DNN) model. Keras runs with Tensorflow in the backend. It's a nice abstraction to start working with NN models. \n",
    "\n",
    "Because Deep Learning is a 1-semester course, we can't talk about each detail about it in the lab session. Here, we only provide a simple template about how to build & run a DL model successfully. You can follow this template to design your model.\n",
    "\n",
    "We will begin by building a fully connected network, which looks like this:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Fully Connected Network](pics/pic1.png)\n",
    "\n",
    "(source: https://github.com/drewnoff/spark-notebook-ml-labs/tree/master/labs/DLFramework)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Prepare data (X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train.shape:  (3613, 500)\n",
      "y_train.shape:  (3613,)\n",
      "X_test.shape:  (347, 500)\n",
      "y_test.shape:  (347,)\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "\n",
    "# standardize name (X, y) \n",
    "X_train = BOW_500.transform(train_df['text'])\n",
    "y_train = train_df['emotion']\n",
    "\n",
    "X_test = BOW_500.transform(test_df['text'])\n",
    "y_test = test_df['emotion']\n",
    "\n",
    "## check dimension is a good habbit \n",
    "print('X_train.shape: ', X_train.shape)\n",
    "print('y_train.shape: ', y_train.shape)\n",
    "print('X_test.shape: ', X_test.shape)\n",
    "print('y_test.shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Deal with categorical label (y)\n",
    "\n",
    "Rather than put your label `train_df['emotion']` directly into a model, we have to process these categorical (or say nominal) label by ourselves. \n",
    "\n",
    "Here, we use the basic method [one-hot encoding](https://en.wikipedia.org/wiki/One-hot) to transform our categorical  labels to numerical ones.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check label:  ['anger' 'fear' 'joy' 'sadness']\n",
      "\n",
      "## Before convert\n",
      "y_train[0:4]:\n",
      " 2245        joy\n",
      "2958    sadness\n",
      "3590    sadness\n",
      "2360        joy\n",
      "Name: emotion, dtype: object\n",
      "\n",
      "y_train.shape:  (3613,)\n",
      "y_test.shape:  (347,)\n",
      "\n",
      "\n",
      "## After convert\n",
      "y_train[0:4]:\n",
      " [[0. 0. 1. 0.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 0. 1.]\n",
      " [0. 0. 1. 0.]]\n",
      "\n",
      "y_train.shape:  (3613, 4)\n",
      "y_test.shape:  (347, 4)\n"
     ]
    }
   ],
   "source": [
    "## deal with label (string -> one-hot)\n",
    "\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder.fit(y_train)\n",
    "print('check label: ', label_encoder.classes_)\n",
    "print('\\n## Before convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n",
    "\n",
    "def label_encode(le, labels):\n",
    "    enc = le.transform(labels)\n",
    "    return keras.utils.to_categorical(enc)\n",
    "\n",
    "def label_decode(le, one_hot_label):\n",
    "    dec = np.argmax(one_hot_label, axis=1)\n",
    "    return le.inverse_transform(dec)\n",
    "\n",
    "y_train = label_encode(label_encoder, y_train)\n",
    "y_test = label_encode(label_encoder, y_test)\n",
    "\n",
    "print('\\n\\n## After convert')\n",
    "print('y_train[0:4]:\\n', y_train[0:4])\n",
    "print('\\ny_train.shape: ', y_train.shape)\n",
    "print('y_test.shape: ', y_test.shape)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_shape:  500\n",
      "output_shape:  4\n"
     ]
    }
   ],
   "source": [
    "# I/O check\n",
    "input_shape = X_train.shape[1]\n",
    "print('input_shape: ', input_shape)\n",
    "\n",
    "output_shape = len(label_encoder.classes_)\n",
    "print('output_shape: ', output_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](pics/pic2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "If using Keras pass *_constraint arguments to layers.\n",
      "Model: \"model_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 64)                32064     \n",
      "_________________________________________________________________\n",
      "re_lu_1 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "re_lu_2 (ReLU)               (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 4)                 260       \n",
      "_________________________________________________________________\n",
      "softmax_1 (Softmax)          (None, 4)                 0         \n",
      "=================================================================\n",
      "Total params: 36,484\n",
      "Trainable params: 36,484\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Model\n",
    "from keras.layers import Input, Dense\n",
    "from keras.layers import ReLU, Softmax\n",
    "\n",
    "# input layer\n",
    "model_input = Input(shape=(input_shape, ))  # 500\n",
    "X = model_input\n",
    "\n",
    "# 1st hidden layer\n",
    "X_W1 = Dense(units=64)(X)  # 64\n",
    "H1 = ReLU()(X_W1)\n",
    "\n",
    "# 2nd hidden layer\n",
    "H1_W2 = Dense(units=64)(H1)  # 64\n",
    "H2 = ReLU()(H1_W2)\n",
    "\n",
    "# output layer\n",
    "H2_W3 = Dense(units=output_shape)(H2)  # 4\n",
    "H3 = Softmax()(H2_W3)\n",
    "\n",
    "model_output = H3\n",
    "\n",
    "# create model\n",
    "model = Model(inputs=[model_input], outputs=[model_output])\n",
    "\n",
    "# loss function & optimizer\n",
    "model.compile(optimizer='adam',\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "# show model construction\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/keras/backend/tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 3613 samples, validate on 347 samples\n",
      "Epoch 1/25\n",
      "3613/3613 [==============================] - 0s 76us/step - loss: 1.3268 - accuracy: 0.3820 - val_loss: 1.2808 - val_accuracy: 0.4294\n",
      "Epoch 2/25\n",
      "3613/3613 [==============================] - 0s 34us/step - loss: 0.9784 - accuracy: 0.6352 - val_loss: 0.8877 - val_accuracy: 0.6859\n",
      "Epoch 3/25\n",
      "3613/3613 [==============================] - 0s 34us/step - loss: 0.5668 - accuracy: 0.7974 - val_loss: 0.7588 - val_accuracy: 0.7089\n",
      "Epoch 4/25\n",
      "3613/3613 [==============================] - 0s 34us/step - loss: 0.3978 - accuracy: 0.8525 - val_loss: 0.7843 - val_accuracy: 0.7147\n",
      "Epoch 5/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.3086 - accuracy: 0.8885 - val_loss: 0.8294 - val_accuracy: 0.6859\n",
      "Epoch 6/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.2466 - accuracy: 0.9145 - val_loss: 0.8636 - val_accuracy: 0.7003\n",
      "Epoch 7/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.2040 - accuracy: 0.9358 - val_loss: 0.9378 - val_accuracy: 0.7147\n",
      "Epoch 8/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.1695 - accuracy: 0.9469 - val_loss: 0.9911 - val_accuracy: 0.6744\n",
      "Epoch 9/25\n",
      "3613/3613 [==============================] - 0s 33us/step - loss: 0.1437 - accuracy: 0.9532 - val_loss: 1.0047 - val_accuracy: 0.7003\n",
      "Epoch 10/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.1295 - accuracy: 0.9604 - val_loss: 1.0870 - val_accuracy: 0.6945\n",
      "Epoch 11/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.1182 - accuracy: 0.9646 - val_loss: 1.1349 - val_accuracy: 0.6859\n",
      "Epoch 12/25\n",
      "3613/3613 [==============================] - 0s 33us/step - loss: 0.1035 - accuracy: 0.9682 - val_loss: 1.1581 - val_accuracy: 0.6744\n",
      "Epoch 13/25\n",
      "3613/3613 [==============================] - 0s 35us/step - loss: 0.0954 - accuracy: 0.9712 - val_loss: 1.2220 - val_accuracy: 0.6801\n",
      "Epoch 14/25\n",
      "3613/3613 [==============================] - 0s 36us/step - loss: 0.0886 - accuracy: 0.9704 - val_loss: 1.2709 - val_accuracy: 0.6801\n",
      "Epoch 15/25\n",
      "3613/3613 [==============================] - 0s 33us/step - loss: 0.0853 - accuracy: 0.9718 - val_loss: 1.3756 - val_accuracy: 0.6744\n",
      "Epoch 16/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.0791 - accuracy: 0.9732 - val_loss: 1.3118 - val_accuracy: 0.6715\n",
      "Epoch 17/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.0720 - accuracy: 0.9756 - val_loss: 1.3723 - val_accuracy: 0.6772\n",
      "Epoch 18/25\n",
      "3613/3613 [==============================] - 0s 36us/step - loss: 0.0723 - accuracy: 0.9743 - val_loss: 1.3759 - val_accuracy: 0.6772\n",
      "Epoch 19/25\n",
      "3613/3613 [==============================] - 0s 34us/step - loss: 0.0673 - accuracy: 0.9765 - val_loss: 1.3918 - val_accuracy: 0.6772\n",
      "Epoch 20/25\n",
      "3613/3613 [==============================] - 0s 35us/step - loss: 0.0629 - accuracy: 0.9759 - val_loss: 1.4067 - val_accuracy: 0.6830\n",
      "Epoch 21/25\n",
      "3613/3613 [==============================] - 0s 35us/step - loss: 0.0693 - accuracy: 0.9756 - val_loss: 1.3826 - val_accuracy: 0.6686\n",
      "Epoch 22/25\n",
      "3613/3613 [==============================] - 0s 36us/step - loss: 0.0591 - accuracy: 0.9787 - val_loss: 1.5240 - val_accuracy: 0.6801\n",
      "Epoch 23/25\n",
      "3613/3613 [==============================] - 0s 33us/step - loss: 0.0565 - accuracy: 0.9768 - val_loss: 1.4911 - val_accuracy: 0.6744\n",
      "Epoch 24/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.0635 - accuracy: 0.9779 - val_loss: 1.5391 - val_accuracy: 0.6686\n",
      "Epoch 25/25\n",
      "3613/3613 [==============================] - 0s 32us/step - loss: 0.0588 - accuracy: 0.9756 - val_loss: 1.5355 - val_accuracy: 0.6628\n",
      "training finish\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import CSVLogger\n",
    "\n",
    "csv_logger = CSVLogger('logs/training_log.csv')\n",
    "\n",
    "# training setting\n",
    "epochs = 25\n",
    "batch_size = 32\n",
    "\n",
    "# training!\n",
    "history = model.fit(X_train, y_train, \n",
    "                    epochs=epochs, \n",
    "                    batch_size=batch_size, \n",
    "                    callbacks=[csv_logger],\n",
    "                    validation_data = (X_test, y_test))\n",
    "print('training finish')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Predict on testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[6.3594210e-01, 1.5577696e-01, 2.0790951e-01, 3.7148903e-04],\n",
       "       [3.8081989e-02, 4.4139552e-01, 7.1226531e-03, 5.1339990e-01],\n",
       "       [2.2235031e-06, 7.0643142e-07, 6.4565003e-01, 3.5434705e-01],\n",
       "       [2.5370235e-02, 3.8255102e-04, 9.4241863e-01, 3.1828590e-02],\n",
       "       [8.9500970e-01, 1.2771991e-05, 1.0496708e-01, 1.0448739e-05]],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## predict\n",
    "pred_result = model.predict(X_test, batch_size=128)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['anger', 'sadness', 'joy', 'joy', 'anger'], dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_result = label_decode(label_encoder, pred_result)\n",
    "pred_result[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing accuracy: 0.66\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "print('testing accuracy: {}'.format(round(accuracy_score(label_decode(label_encoder, y_test), pred_result), 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>accuracy</th>\n",
       "      <th>loss</th>\n",
       "      <th>val_accuracy</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0.381954</td>\n",
       "      <td>1.326823</td>\n",
       "      <td>0.429395</td>\n",
       "      <td>1.280761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.635206</td>\n",
       "      <td>0.978438</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>0.887704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0.797398</td>\n",
       "      <td>0.566829</td>\n",
       "      <td>0.708934</td>\n",
       "      <td>0.758821</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0.852477</td>\n",
       "      <td>0.397790</td>\n",
       "      <td>0.714697</td>\n",
       "      <td>0.784289</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>0.888458</td>\n",
       "      <td>0.308585</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>0.829405</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>0.914475</td>\n",
       "      <td>0.246630</td>\n",
       "      <td>0.700288</td>\n",
       "      <td>0.863588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>0.935787</td>\n",
       "      <td>0.203988</td>\n",
       "      <td>0.714697</td>\n",
       "      <td>0.937775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0.946859</td>\n",
       "      <td>0.169463</td>\n",
       "      <td>0.674352</td>\n",
       "      <td>0.991099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0.953225</td>\n",
       "      <td>0.143732</td>\n",
       "      <td>0.700288</td>\n",
       "      <td>1.004707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>9</td>\n",
       "      <td>0.960421</td>\n",
       "      <td>0.129482</td>\n",
       "      <td>0.694524</td>\n",
       "      <td>1.086991</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>0.964572</td>\n",
       "      <td>0.118229</td>\n",
       "      <td>0.685879</td>\n",
       "      <td>1.134904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0.968171</td>\n",
       "      <td>0.103541</td>\n",
       "      <td>0.674352</td>\n",
       "      <td>1.158057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>0.971215</td>\n",
       "      <td>0.095421</td>\n",
       "      <td>0.680115</td>\n",
       "      <td>1.221972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>13</td>\n",
       "      <td>0.970385</td>\n",
       "      <td>0.088575</td>\n",
       "      <td>0.680115</td>\n",
       "      <td>1.270906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>14</td>\n",
       "      <td>0.971769</td>\n",
       "      <td>0.085306</td>\n",
       "      <td>0.674352</td>\n",
       "      <td>1.375648</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>15</td>\n",
       "      <td>0.973152</td>\n",
       "      <td>0.079097</td>\n",
       "      <td>0.671470</td>\n",
       "      <td>1.311819</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>16</td>\n",
       "      <td>0.975643</td>\n",
       "      <td>0.072036</td>\n",
       "      <td>0.677233</td>\n",
       "      <td>1.372287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>17</td>\n",
       "      <td>0.974260</td>\n",
       "      <td>0.072258</td>\n",
       "      <td>0.677233</td>\n",
       "      <td>1.375895</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>18</td>\n",
       "      <td>0.976474</td>\n",
       "      <td>0.067268</td>\n",
       "      <td>0.677233</td>\n",
       "      <td>1.391781</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>19</td>\n",
       "      <td>0.975920</td>\n",
       "      <td>0.062896</td>\n",
       "      <td>0.682997</td>\n",
       "      <td>1.406693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>20</td>\n",
       "      <td>0.975643</td>\n",
       "      <td>0.069344</td>\n",
       "      <td>0.668588</td>\n",
       "      <td>1.382614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>21</td>\n",
       "      <td>0.978688</td>\n",
       "      <td>0.059102</td>\n",
       "      <td>0.680115</td>\n",
       "      <td>1.523984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>22</td>\n",
       "      <td>0.976751</td>\n",
       "      <td>0.056523</td>\n",
       "      <td>0.674352</td>\n",
       "      <td>1.491136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>23</td>\n",
       "      <td>0.977858</td>\n",
       "      <td>0.063490</td>\n",
       "      <td>0.668588</td>\n",
       "      <td>1.539128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>24</td>\n",
       "      <td>0.975643</td>\n",
       "      <td>0.058752</td>\n",
       "      <td>0.662824</td>\n",
       "      <td>1.535484</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    epoch  accuracy      loss  val_accuracy  val_loss\n",
       "0       0  0.381954  1.326823      0.429395  1.280761\n",
       "1       1  0.635206  0.978438      0.685879  0.887704\n",
       "2       2  0.797398  0.566829      0.708934  0.758821\n",
       "3       3  0.852477  0.397790      0.714697  0.784289\n",
       "4       4  0.888458  0.308585      0.685879  0.829405\n",
       "5       5  0.914475  0.246630      0.700288  0.863588\n",
       "6       6  0.935787  0.203988      0.714697  0.937775\n",
       "7       7  0.946859  0.169463      0.674352  0.991099\n",
       "8       8  0.953225  0.143732      0.700288  1.004707\n",
       "9       9  0.960421  0.129482      0.694524  1.086991\n",
       "10     10  0.964572  0.118229      0.685879  1.134904\n",
       "11     11  0.968171  0.103541      0.674352  1.158057\n",
       "12     12  0.971215  0.095421      0.680115  1.221972\n",
       "13     13  0.970385  0.088575      0.680115  1.270906\n",
       "14     14  0.971769  0.085306      0.674352  1.375648\n",
       "15     15  0.973152  0.079097      0.671470  1.311819\n",
       "16     16  0.975643  0.072036      0.677233  1.372287\n",
       "17     17  0.974260  0.072258      0.677233  1.375895\n",
       "18     18  0.976474  0.067268      0.677233  1.391781\n",
       "19     19  0.975920  0.062896      0.682997  1.406693\n",
       "20     20  0.975643  0.069344      0.668588  1.382614\n",
       "21     21  0.978688  0.059102      0.680115  1.523984\n",
       "22     22  0.976751  0.056523      0.674352  1.491136\n",
       "23     23  0.977858  0.063490      0.668588  1.539128\n",
       "24     24  0.975643  0.058752      0.662824  1.535484"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Let's take a look at the training log\n",
    "training_log = pd.DataFrame()\n",
    "training_log = pd.read_csv(\"logs/training_log.csv\")\n",
    "training_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 6 (Take home): **  \n",
    "\n",
    "Plot the Training and Validation Accuracy and Loss (different plots), just like the images below (Note: the pictures below are an example from a different model). How to interpret the graphs you got? How are they related to the concept of overfitting/underfitting covered in class?\n",
    "<table><tr>\n",
    "    <td><img src=\"pics/pic3.png\" style=\"width: 300px;\"/> </td>\n",
    "    <td><img src=\"pics/pic4.png\" style=\"width: 300px;\"/> </td>\n",
    "</tr></table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note\n",
    "\n",
    "If you don't have a GPU (level is higher than GTX 1060) or you are not good at setting lots of things about computer, we recommend you to use the [kaggle kernel](https://www.kaggle.com/kernels) to do deep learning model training. They have already installed all the librarys and provided free GPU for you to use.\n",
    "\n",
    "Note however that you will only be able to run a kernel for 6 hours. After 6 hours of inactivity, your Kaggle kernel will shut down (meaning if your model takes more than 6 hours to train, you can't train it at once).\n",
    "\n",
    "\n",
    "### More Information for your reference\n",
    "\n",
    "* Keras document: https://keras.io/\n",
    "* Keras GitHub example: https://github.com/keras-team/keras/tree/master/examples\n",
    "* CS229: Machine Learning: http://cs229.stanford.edu/syllabus.html\n",
    "* Deep Learning cheatsheet: https://stanford.edu/~shervine/teaching/cs-229/cheatsheet-deep-learning\n",
    "* If you want to try TensorFlow or PyTorch: https://pytorch.org/tutorials/\n",
    "https://www.tensorflow.org/tutorials/quickstart/beginner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Word2Vector\n",
    "\n",
    "We will introduce how to use `gensim` to train your word2vec model and how to load a pre-trained model.\n",
    "\n",
    "https://radimrehurek.com/gensim/index.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Prepare training corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2245</td>\n",
       "      <td>30241</td>\n",
       "      <td>What if.... the Metro LRT went over the Walter...</td>\n",
       "      <td>[What, if, ..., ., the, Metro, LRT, went, over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2958</td>\n",
       "      <td>40131</td>\n",
       "      <td>@HannahFJames I'm distraught! 😭 Candice and he...</td>\n",
       "      <td>[@, HannahFJames, I, 'm, distraught, !, 😭, Can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>40763</td>\n",
       "      <td>@urbaneprofessor roast them. Then risotto with...</td>\n",
       "      <td>[@, urbaneprofessor, roast, them, ., Then, ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>30356</td>\n",
       "      <td>Transitioning to a new job is hard when you ha...</td>\n",
       "      <td>[Transitioning, to, a, new, job, is, hard, whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905</td>\n",
       "      <td>21048</td>\n",
       "      <td>Don't be so shy #nl</td>\n",
       "      <td>[Do, n't, be, so, shy, #, nl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "2245  30241  What if.... the Metro LRT went over the Walter...   \n",
       "2958  40131  @HannahFJames I'm distraught! 😭 Candice and he...   \n",
       "3590  40763  @urbaneprofessor roast them. Then risotto with...   \n",
       "2360  30356  Transitioning to a new job is hard when you ha...   \n",
       "1905  21048                                Don't be so shy #nl   \n",
       "\n",
       "                                         text_tokenized  \n",
       "2245  [What, if, ..., ., the, Metro, LRT, went, over...  \n",
       "2958  [@, HannahFJames, I, 'm, distraught, !, 😭, Can...  \n",
       "3590  [@, urbaneprofessor, roast, them, ., Then, ris...  \n",
       "2360  [Transitioning, to, a, new, job, is, hard, whe...  \n",
       "1905                      [Do, n't, be, so, shy, #, nl]  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## check library\n",
    "import gensim\n",
    "\n",
    "## ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# # if you want to see the training messages, you can use it\n",
    "# import logging\n",
    "# logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['What', 'if', '...', '.', 'the', 'Metro', 'LRT', 'went', 'over', 'the', 'Walterdale', '?', '!', '?', '!', '😂', '#', 'yeg']),\n",
       "       list(['@', 'HannahFJames', 'I', \"'m\", 'distraught', '!', '😭', 'Candice', 'and', 'her', 'pout', 'can', 'piss', 'off']),\n",
       "       list(['@', 'urbaneprofessor', 'roast', 'them', '.', 'Then', 'risotto', 'with', 'sage', 'and', 'pine', 'nuts'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create the training corpus\n",
    "training_corpus = train_df['text_tokenized'].values\n",
    "training_corpus[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.2 Training our model\n",
    "\n",
    "You can try to train your own model. More details: https://radimrehurek.com/gensim/models/word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>text_tokenized</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>2245</td>\n",
       "      <td>30241</td>\n",
       "      <td>What if.... the Metro LRT went over the Walter...</td>\n",
       "      <td>[What, if, ..., ., the, Metro, LRT, went, over...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2958</td>\n",
       "      <td>40131</td>\n",
       "      <td>@HannahFJames I'm distraught! 😭 Candice and he...</td>\n",
       "      <td>[@, HannahFJames, I, 'm, distraught, !, 😭, Can...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3590</td>\n",
       "      <td>40763</td>\n",
       "      <td>@urbaneprofessor roast them. Then risotto with...</td>\n",
       "      <td>[@, urbaneprofessor, roast, them, ., Then, ris...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2360</td>\n",
       "      <td>30356</td>\n",
       "      <td>Transitioning to a new job is hard when you ha...</td>\n",
       "      <td>[Transitioning, to, a, new, job, is, hard, whe...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1905</td>\n",
       "      <td>21048</td>\n",
       "      <td>Don't be so shy #nl</td>\n",
       "      <td>[Do, n't, be, so, shy, #, nl]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  \\\n",
       "2245  30241  What if.... the Metro LRT went over the Walter...   \n",
       "2958  40131  @HannahFJames I'm distraught! 😭 Candice and he...   \n",
       "3590  40763  @urbaneprofessor roast them. Then risotto with...   \n",
       "2360  30356  Transitioning to a new job is hard when you ha...   \n",
       "1905  21048                                Don't be so shy #nl   \n",
       "\n",
       "                                         text_tokenized  \n",
       "2245  [What, if, ..., ., the, Metro, LRT, went, over...  \n",
       "2958  [@, HannahFJames, I, 'm, distraught, !, 😭, Can...  \n",
       "3590  [@, urbaneprofessor, roast, them, ., Then, ris...  \n",
       "2360  [Transitioning, to, a, new, job, is, hard, whe...  \n",
       "1905                      [Do, n't, be, so, shy, #, nl]  "
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## the input type\n",
    "train_df['text_tokenized'] = train_df['text'].apply(lambda x: nltk.word_tokenize(x))\n",
    "train_df[['id', 'text', 'text_tokenized']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "## setting\n",
    "vector_dim = 100\n",
    "window_size = 5\n",
    "min_count = 1\n",
    "training_iter = 20\n",
    "\n",
    "## model\n",
    "word2vec_model = Word2Vec(sentences=training_corpus, \n",
    "                          size=vector_dim, window=window_size, \n",
    "                          min_count=min_count, iter=training_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](https://i.imgur.com/Fca3MCs.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.3 Generating word vector (embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 8.6363715e-01,  2.8205007e-01,  3.9957294e-01, -4.0624791e-01,\n",
       "       -3.4555924e-01, -7.1382445e-01,  3.7479270e-01,  2.7684513e-03,\n",
       "        4.6583772e-01,  2.0052099e-01, -9.7175635e-02,  1.9687834e-01,\n",
       "        7.5468493e-01, -2.3295599e-01,  9.6613560e-03, -2.7952102e-01,\n",
       "       -4.4478896e-01, -1.0125370e+00,  2.4720362e-01,  4.1536635e-01,\n",
       "       -2.3638040e-01, -8.4537223e-02,  9.0714639e-01, -1.1353905e+00,\n",
       "        2.7562323e-01,  4.8880211e-01,  2.9663628e-02, -3.8980278e-01,\n",
       "        5.1371276e-01,  3.5615084e-01,  1.3454179e-01,  6.1493265e-03,\n",
       "       -7.7622049e-02,  1.0455801e+00, -8.8339281e-01,  3.0487704e-01,\n",
       "        2.7665254e-01,  4.2238656e-01, -8.4530123e-02, -1.0092280e+00,\n",
       "        3.8103780e-01, -2.1789047e-01,  1.0305718e+00, -5.1571697e-01,\n",
       "        2.4597752e-01, -1.1658813e-01, -9.5989317e-01, -3.2880595e-01,\n",
       "       -1.7900856e-01,  2.9847324e-01, -1.7239939e-01, -1.0750273e-01,\n",
       "        1.1161696e+00,  3.5833004e-01,  2.4887963e-01,  3.9707342e-01,\n",
       "        4.1809008e-01, -1.6108392e-01,  7.3490542e-01,  1.0192102e+00,\n",
       "        2.5798130e-01,  1.0302427e-01, -6.8302780e-01,  3.8136989e-01,\n",
       "        7.4553663e-01, -4.1347426e-01,  8.1126593e-02, -1.8992722e-02,\n",
       "        1.0268439e-03,  6.4451700e-01,  1.0886976e-01, -2.3654784e-01,\n",
       "        9.4672352e-01, -5.9164554e-01,  5.6875014e-01,  6.1377585e-01,\n",
       "        1.7445793e-02, -6.2606543e-01, -6.2884462e-01,  3.4593138e-01,\n",
       "       -4.0940887e-01,  3.4066272e-01,  4.9331796e-01, -1.3414177e-01,\n",
       "       -3.8250834e-01, -2.1791911e-01, -6.2767047e-01,  3.0479079e-01,\n",
       "        8.2202715e-01,  7.6642680e-01, -5.5233184e-02, -2.1688019e-01,\n",
       "       -3.6340025e-01,  2.7430660e-01, -1.1989570e+00,  5.7648104e-01,\n",
       "       -5.1935160e-01,  5.1181279e-02,  5.5125397e-01,  2.9705424e-02],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the corresponding vector of a word\n",
    "word_vec = word2vec_model.wv['happy']\n",
    "word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('bday', 0.9530141353607178),\n",
       " ('O', 0.9517824649810791),\n",
       " (\"'you\", 0.9465159177780151),\n",
       " ('birthday', 0.9425859451293945),\n",
       " ('Be', 0.9371531009674072),\n",
       " ('free', 0.9347186088562012),\n",
       " ('suffer', 0.9331982731819153),\n",
       " ('Online', 0.9291149377822876),\n",
       " ('plz', 0.9278095960617065),\n",
       " ('establishment', 0.9276406764984131)]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the most similar words\n",
    "word = 'happy'\n",
    "topn = 10\n",
    "word2vec_model.most_similar(word, topn=topn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.4 Using a pre-trained w2v model\n",
    "\n",
    "Instead of training your own model ,you can use a model that has already been trained. Here, we see 2 ways of doing that:\n",
    "\n",
    "\n",
    "#### (1) Download model by yourself\n",
    "\n",
    "source: [GoogleNews-vectors-negative300](https://code.google.com/archive/p/word2vec/)\n",
    "\n",
    "more details: https://radimrehurek.com/gensim/models/keyedvectors.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'GoogleNews/GoogleNews-vectors-negative300.bin.gz'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-45-79b487053ea3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;31m## Note: this model is very huge, this will take some time ...\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mmodel_path\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mw2v_google_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKeyedVectors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_word2vec_format\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'load ok'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/keyedvectors.py\u001b[0m in \u001b[0;36mload_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m   1496\u001b[0m         return _load_word2vec_format(\n\u001b[1;32m   1497\u001b[0m             \u001b[0mcls\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfvocab\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfvocab\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0municode_errors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0municode_errors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1498\u001b[0;31m             limit=limit, datatype=datatype)\n\u001b[0m\u001b[1;32m   1499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_keras_embedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_embeddings\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/gensim/models/utils_any2vec.py\u001b[0m in \u001b[0;36m_load_word2vec_format\u001b[0;34m(cls, fname, fvocab, binary, encoding, unicode_errors, limit, datatype)\u001b[0m\n\u001b[1;32m    340\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfo\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"loading projection weights from %s\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 342\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfin\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mheader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_unicode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfin\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0mvocab_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvector_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mheader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# throws for invalid file format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(uri, mode, buffering, encoding, errors, newline, closefd, opener, ignore_ext, transport_params)\u001b[0m\n\u001b[1;32m    350\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    351\u001b[0m         \u001b[0mbinary_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 352\u001b[0;31m     \u001b[0mbinary\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_binary_stream\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muri\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbinary_mode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtransport_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    353\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mignore_ext\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    354\u001b[0m         \u001b[0mdecompressed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbinary\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.7/lib/python3.7/site-packages/smart_open/smart_open_lib.py\u001b[0m in \u001b[0;36m_open_binary_stream\u001b[0;34m(uri, mode, transport_params)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"file\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 548\u001b[0;31m             \u001b[0mfobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muri_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    549\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mparsed_uri\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheme\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msmart_open_ssh\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSCHEMES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'GoogleNews/GoogleNews-vectors-negative300.bin.gz'"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors\n",
    "\n",
    "## Note: this model is very huge, this will take some time ...\n",
    "model_path = \"GoogleNews/GoogleNews-vectors-negative300.bin.gz\"\n",
    "w2v_google_model = KeyedVectors.load_word2vec_format(model_path, binary=True)\n",
    "print('load ok')\n",
    "\n",
    "w2v_google_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### (2) Using gensim api\n",
    "\n",
    "Other pretrained models are available here: https://github.com/RaRe-Technologies/gensim-data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 104.8/104.8MB downloaded\n",
      "load ok\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[('birthday', 0.9577817916870117),\n",
       " ('thank', 0.9376667141914368),\n",
       " ('welcome', 0.9336150288581848),\n",
       " ('love', 0.9176183938980103),\n",
       " ('miss', 0.916450023651123),\n",
       " ('hello', 0.9158351421356201),\n",
       " ('thanks', 0.9150084257125854),\n",
       " ('merry', 0.9053248763084412),\n",
       " ('bless', 0.9027323126792908),\n",
       " ('wish', 0.9013165831565857)]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import gensim.downloader as api\n",
    "\n",
    "## If you see `SSL: CERTIFICATE_VERIFY_FAILED` error, use this:\n",
    "import ssl\n",
    "import urllib.request\n",
    "ssl._create_default_https_context = ssl._create_unverified_context\n",
    "\n",
    "glove_twitter_25_model = api.load(\"glove-twitter-25\")\n",
    "print('load ok')\n",
    "\n",
    "glove_twitter_25_model.most_similar('happy', topn=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.5 king + woman - man = ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's run one of the most famous examples for Word2Vec and compute the similarity between these 3 words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('queen', 0.7118192911148071),\n",
       " ('monarch', 0.6189674139022827),\n",
       " ('princess', 0.5902431011199951),\n",
       " ('crown_prince', 0.5499460697174072),\n",
       " ('prince', 0.5377321243286133),\n",
       " ('kings', 0.5236844420433044),\n",
       " ('Queen_Consort', 0.5235945582389832),\n",
       " ('queens', 0.518113374710083),\n",
       " ('sultan', 0.5098593235015869),\n",
       " ('monarchy', 0.5087411999702454)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v_google_model.most_similar(positive=['king', 'woman'], negative=['man'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 7 (Take home): **  \n",
    "\n",
    "Now, we have the word vectors, but our input data is a sequence of words (or say sentence). \n",
    "How can we utilize these \"word\" vectors to represent the sentence data and train our model?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Clustering: k-means\n",
    "\n",
    "Here we introduce how to use `sklearn` to do the basic **unsupervised learning** approach, k-means.    \n",
    "\n",
    "more details: http://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic concept\n",
    "\n",
    "![Image](https://i.imgur.com/PEdUf54.png)\n",
    "\n",
    "(img source: https://towardsdatascience.com/k-means-clustering-identifying-f-r-i-e-n-d-s-in-the-world-of-strangers-695537505d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target words:  ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\n"
     ]
    }
   ],
   "source": [
    "# clustering target\n",
    "target_list = ['happy', 'fear', 'angry', 'car', 'teacher', 'computer']\n",
    "print('target words: ', target_list)\n",
    "\n",
    "# convert to word vector\n",
    "X = [word2vec_model.wv[word] for word in target_list]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word: happy \t cluster: 0\n",
      "word: fear \t cluster: 0\n",
      "word: angry \t cluster: 0\n",
      "word: car \t cluster: 1\n",
      "word: teacher \t cluster: 1\n",
      "word: computer \t cluster: 1\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# we have to decide how many cluster (k) we want\n",
    "k = 2\n",
    "\n",
    "# k-means model\n",
    "kmeans_model = KMeans(n_clusters=k)\n",
    "kmeans_model.fit(X)\n",
    "\n",
    "# cluster result\n",
    "cluster_result = kmeans_model.labels_\n",
    "\n",
    "# show\n",
    "for i in range(len(target_list)):\n",
    "    print('word: {} \\t cluster: {}'.format(target_list[i], cluster_result[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Imgur](pics/pic6.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1], dtype=int32)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cluster membership\n",
    "word = 'student'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0], dtype=int32)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check cluster membership\n",
    "word = 'sad'\n",
    "word_vec = word2vec_model.wv[word]\n",
    "kmeans_model.predict([word_vec])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## 9. High-dimension Visualization: t-SNE\n",
    "\n",
    "No matter if you use the Bag-of-words, tf-idf, or word2vec, it's very hard to see the embedding result, because the dimension is larger than 3.  \n",
    "\n",
    "In Lab 1, we already talked about PCA. We can use PCA to reduce the dimension of our data, then visualize it. However, if you dig deeper into the result, you'd find it is insufficient...\n",
    "\n",
    "Our aim will be to create a visualization similar to the one below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](pics/pic7.png)\n",
    "source: https://www.fabian-keller.de/research/high-dimensional-data-visualization "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we would like to introduce another visualization method called t-SNE.  \n",
    "http://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.html "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Prepare visualizing target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's repare data lists like:\n",
    "    - happpy words\n",
    "    - angry words\n",
    "    - data words\n",
    "    - mining words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_google_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-51-19343c27d4c0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtopn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mhappy_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'happy'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'happy'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mangry_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'angry'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'angry'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mdata_words\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mword_\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msim_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_google_model' is not defined"
     ]
    }
   ],
   "source": [
    "word_list = ['happy', 'angry', 'data', 'mining']\n",
    "\n",
    "topn = 5\n",
    "happy_words = ['happy'] + [word_ for word_, sim_ in w2v_google_model.most_similar('happy', topn=topn)]\n",
    "angry_words = ['angry'] + [word_ for word_, sim_ in w2v_google_model.most_similar('angry', topn=topn)]        \n",
    "data_words = ['data'] + [word_ for word_, sim_ in w2v_google_model.most_similar('data', topn=topn)]        \n",
    "mining_words = ['mining'] + [word_ for word_, sim_ in w2v_google_model.most_similar('mining', topn=topn)]        \n",
    "\n",
    "print('happy_words: ', happy_words)\n",
    "print('angry_words: ', angry_words)\n",
    "print('data_words: ', data_words)\n",
    "print('mining_words: ', mining_words)\n",
    "\n",
    "target_words = happy_words + angry_words + data_words + mining_words\n",
    "print('\\ntarget words: ')\n",
    "print(target_words)\n",
    "\n",
    "print('\\ncolor list:')\n",
    "cn = topn + 1\n",
    "color = ['b'] * cn + ['g'] * cn + ['r'] * cn + ['y'] * cn\n",
    "print(color)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9.1 Plot using t-SNE (2-dimension)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w2v_google_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-52-721c47ff190f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m## w2v model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mw2v_google_model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m## prepare training word vectors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'w2v_google_model' is not defined"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "## w2v model\n",
    "model = w2v_google_model\n",
    "\n",
    "## prepare training word vectors\n",
    "size = 200\n",
    "target_size = len(target_words)\n",
    "all_word = list(model.vocab.keys())\n",
    "word_train = target_words + all_word[:size]\n",
    "X_train = model[word_train]\n",
    "\n",
    "## t-SNE model\n",
    "tsne = TSNE(n_components=2, metric='cosine', random_state=28)\n",
    "\n",
    "## training\n",
    "X_tsne = tsne.fit_transform(X_train)\n",
    "\n",
    "## plot the result\n",
    "plt.figure(figsize=(7.5, 7.5), dpi=115)\n",
    "plt.scatter(X_tsne[:target_size, 0], X_tsne[:target_size, 1], c=color)\n",
    "for label, x, y in zip(target_words, X_tsne[:target_size, 0], X_tsne[:target_size, 1]):\n",
    "    plt.annotate(label, xy=(x,y), xytext=(0,0),  textcoords='offset points')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### ** >>> Exercise 8 (Take home): **  \n",
    "\n",
    "Generate a t-SNE visualization to show the 15 words most related to the words \"angry\", \"happy\", \"sad\", \"fear\" (60 words total)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Answer here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Elmo embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Besides Word2Vec, several other pretrained models for generating embeddings exist. Here, we'll take a look at ElMo embeddings.\n",
    "Elmo is a language model trained on a task to predict the next word in a sequence of words, but is bidirectional (unlike word2vec).\n",
    "\n",
    "[Image](pic/pics8.png)\n",
    "\n",
    "Source: (http://jalammar.github.io/illustrated-bert/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed the sentences, we need to tokenize them and get them into a tensor of uniform shape.This means every sentence should have the same amount of tokens  (ie. If we have 5 sentences, we should have an array of 5 * x ). We will achieve this through padding, or adding \"\" at the end of each sentence for each missing token up to x.\n",
    "\n",
    "We'll be using the Keras tokenizer to tokenize and pad. Keras tokenizer will first map each word to a number, and we'll get the tokenizing below in the text_tok_keras column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tok_keras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>10490</td>\n",
       "      <td>Take public opinion on revenge with Pakistan i...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.458</td>\n",
       "      <td>[Take, public, opinion, on, revenge, with, Pak...</td>\n",
       "      <td>[208, 1187, 1424, 14, 209, 22, 282, 33, 2509, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>21018</td>\n",
       "      <td>My goals are so big they scare small minds</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.250</td>\n",
       "      <td>[My, goals, are, so, big, they, scare, small, ...</td>\n",
       "      <td>[10, 757, 28, 18, 340, 51, 480, 1425, 1426]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>20289</td>\n",
       "      <td>@PanicAtTheDisco hey, y'all announced it like ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[@, PanicAtTheDisco, hey, ,, y'all, announced,...</td>\n",
       "      <td>[1427, 517, 518, 2512, 12, 27, 1428, 110, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2878</td>\n",
       "      <td>40051</td>\n",
       "      <td>@Christy_RTR @doge_e_fresh I'm despondent</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.806</td>\n",
       "      <td>[@, Christy_RTR, @, doge_e_fresh, I, 'm, despo...</td>\n",
       "      <td>[4991, 4992, 25, 2513]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>10432</td>\n",
       "      <td>@sarah_urbina why do you even beef Sara you le...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[@, sarah_urbina, why, do, you, even, beef, Sa...</td>\n",
       "      <td>[4993, 78, 50, 8, 114, 1813, 4994, 8, 138, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  emotion  \\\n",
       "490   10490  Take public opinion on revenge with Pakistan i...    anger   \n",
       "1875  21018         My goals are so big they scare small minds     fear   \n",
       "1146  20289  @PanicAtTheDisco hey, y'all announced it like ...     fear   \n",
       "2878  40051          @Christy_RTR @doge_e_fresh I'm despondent  sadness   \n",
       "432   10432  @sarah_urbina why do you even beef Sara you le...    anger   \n",
       "\n",
       "      intensity                                     text_tokenized  \\\n",
       "490       0.458  [Take, public, opinion, on, revenge, with, Pak...   \n",
       "1875      0.250  [My, goals, are, so, big, they, scare, small, ...   \n",
       "1146      0.625  [@, PanicAtTheDisco, hey, ,, y'all, announced,...   \n",
       "2878      0.806  [@, Christy_RTR, @, doge_e_fresh, I, 'm, despo...   \n",
       "432       0.479  [@, sarah_urbina, why, do, you, even, beef, Sa...   \n",
       "\n",
       "                                         text_tok_keras  \n",
       "490   [208, 1187, 1424, 14, 209, 22, 282, 33, 2509, ...  \n",
       "1875        [10, 757, 28, 18, 340, 51, 480, 1425, 1426]  \n",
       "1146  [1427, 517, 518, 2512, 12, 27, 1428, 110, 3, 3...  \n",
       "2878                             [4991, 4992, 25, 2513]  \n",
       "432   [4993, 78, 50, 8, 114, 1813, 4994, 8, 138, 1, ...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "#Initializing tokenizer, getting rid of some punctuation\n",
    "tokenizer_keras = Tokenizer(filters='\"#%&()*+,-./:;<=>@[\\]^`{|}~')\n",
    "tokenizer_keras.fit_on_texts(train_df['text'])\n",
    "train_df['text_tok_keras'] = tokenizer_keras.texts_to_sequences(train_df['text'])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[16, 9, 14, 4, 24]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the lenght of the tokenized sentences\n",
    "list(map(lambda x: len(x), train_df['text_tok_keras'].iloc[:5]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we discudded, the lenght of the tokenized sentences is not the same, so we pad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tok_keras</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>10490</td>\n",
       "      <td>Take public opinion on revenge with Pakistan i...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.458</td>\n",
       "      <td>[Take, public, opinion, on, revenge, with, Pak...</td>\n",
       "      <td>[208, 1187, 1424, 14, 209, 22, 282, 33, 2509, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>21018</td>\n",
       "      <td>My goals are so big they scare small minds</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.250</td>\n",
       "      <td>[My, goals, are, so, big, they, scare, small, ...</td>\n",
       "      <td>[10, 757, 28, 18, 340, 51, 480, 1425, 1426, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>20289</td>\n",
       "      <td>@PanicAtTheDisco hey, y'all announced it like ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[@, PanicAtTheDisco, hey, ,, y'all, announced,...</td>\n",
       "      <td>[1427, 517, 518, 2512, 12, 27, 1428, 110, 3, 3...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2878</td>\n",
       "      <td>40051</td>\n",
       "      <td>@Christy_RTR @doge_e_fresh I'm despondent</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.806</td>\n",
       "      <td>[@, Christy_RTR, @, doge_e_fresh, I, 'm, despo...</td>\n",
       "      <td>[4991, 4992, 25, 2513, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>10432</td>\n",
       "      <td>@sarah_urbina why do you even beef Sara you le...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[@, sarah_urbina, why, do, you, even, beef, Sa...</td>\n",
       "      <td>[4993, 78, 50, 8, 114, 1813, 4994, 8, 138, 1, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  emotion  \\\n",
       "490   10490  Take public opinion on revenge with Pakistan i...    anger   \n",
       "1875  21018         My goals are so big they scare small minds     fear   \n",
       "1146  20289  @PanicAtTheDisco hey, y'all announced it like ...     fear   \n",
       "2878  40051          @Christy_RTR @doge_e_fresh I'm despondent  sadness   \n",
       "432   10432  @sarah_urbina why do you even beef Sara you le...    anger   \n",
       "\n",
       "      intensity                                     text_tokenized  \\\n",
       "490       0.458  [Take, public, opinion, on, revenge, with, Pak...   \n",
       "1875      0.250  [My, goals, are, so, big, they, scare, small, ...   \n",
       "1146      0.625  [@, PanicAtTheDisco, hey, ,, y'all, announced,...   \n",
       "2878      0.806  [@, Christy_RTR, @, doge_e_fresh, I, 'm, despo...   \n",
       "432       0.479  [@, sarah_urbina, why, do, you, even, beef, Sa...   \n",
       "\n",
       "                                         text_tok_keras  \n",
       "490   [208, 1187, 1424, 14, 209, 22, 282, 33, 2509, ...  \n",
       "1875  [10, 757, 28, 18, 340, 51, 480, 1425, 1426, 0,...  \n",
       "1146  [1427, 517, 518, 2512, 12, 27, 1428, 110, 3, 3...  \n",
       "2878  [4991, 4992, 25, 2513, 0, 0, 0, 0, 0, 0, 0, 0,...  \n",
       "432   [4993, 78, 50, 8, 114, 1813, 4994, 8, 138, 1, ...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "#pad sequences\n",
    "maxlen = 30 # arbitrary\n",
    "padded_tokens = pad_sequences(train_df['text_tok_keras'],  maxlen=maxlen, padding=\"post\")\n",
    "train_df['text_tok_keras'] = list(padded_tokens)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we map back to words to obtain the padded tokenized representations in the text_tok_keras_words column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>emotion</th>\n",
       "      <th>intensity</th>\n",
       "      <th>text_tokenized</th>\n",
       "      <th>text_tok_keras</th>\n",
       "      <th>text_tok_keras_words</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>490</td>\n",
       "      <td>10490</td>\n",
       "      <td>Take public opinion on revenge with Pakistan i...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.458</td>\n",
       "      <td>[Take, public, opinion, on, revenge, with, Pak...</td>\n",
       "      <td>[208, 1187, 1424, 14, 209, 22, 282, 33, 2509, ...</td>\n",
       "      <td>[take, public, opinion, on, revenge, with, pak...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1875</td>\n",
       "      <td>21018</td>\n",
       "      <td>My goals are so big they scare small minds</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.250</td>\n",
       "      <td>[My, goals, are, so, big, they, scare, small, ...</td>\n",
       "      <td>[10, 757, 28, 18, 340, 51, 480, 1425, 1426, 0,...</td>\n",
       "      <td>[my, goals, are, so, big, they, scare, small, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1146</td>\n",
       "      <td>20289</td>\n",
       "      <td>@PanicAtTheDisco hey, y'all announced it like ...</td>\n",
       "      <td>fear</td>\n",
       "      <td>0.625</td>\n",
       "      <td>[@, PanicAtTheDisco, hey, ,, y'all, announced,...</td>\n",
       "      <td>[1427, 517, 518, 2512, 12, 27, 1428, 110, 3, 3...</td>\n",
       "      <td>[panicatthedisco, hey, y'all, announced, it, l...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2878</td>\n",
       "      <td>40051</td>\n",
       "      <td>@Christy_RTR @doge_e_fresh I'm despondent</td>\n",
       "      <td>sadness</td>\n",
       "      <td>0.806</td>\n",
       "      <td>[@, Christy_RTR, @, doge_e_fresh, I, 'm, despo...</td>\n",
       "      <td>[4991, 4992, 25, 2513, 0, 0, 0, 0, 0, 0, 0, 0,...</td>\n",
       "      <td>[christy_rtr, doge_e_fresh, i'm, despondent, ,...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>432</td>\n",
       "      <td>10432</td>\n",
       "      <td>@sarah_urbina why do you even beef Sara you le...</td>\n",
       "      <td>anger</td>\n",
       "      <td>0.479</td>\n",
       "      <td>[@, sarah_urbina, why, do, you, even, beef, Sa...</td>\n",
       "      <td>[4993, 78, 50, 8, 114, 1813, 4994, 8, 138, 1, ...</td>\n",
       "      <td>[sarah_urbina, why, do, you, even, beef, sara,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         id                                               text  emotion  \\\n",
       "490   10490  Take public opinion on revenge with Pakistan i...    anger   \n",
       "1875  21018         My goals are so big they scare small minds     fear   \n",
       "1146  20289  @PanicAtTheDisco hey, y'all announced it like ...     fear   \n",
       "2878  40051          @Christy_RTR @doge_e_fresh I'm despondent  sadness   \n",
       "432   10432  @sarah_urbina why do you even beef Sara you le...    anger   \n",
       "\n",
       "      intensity                                     text_tokenized  \\\n",
       "490       0.458  [Take, public, opinion, on, revenge, with, Pak...   \n",
       "1875      0.250  [My, goals, are, so, big, they, scare, small, ...   \n",
       "1146      0.625  [@, PanicAtTheDisco, hey, ,, y'all, announced,...   \n",
       "2878      0.806  [@, Christy_RTR, @, doge_e_fresh, I, 'm, despo...   \n",
       "432       0.479  [@, sarah_urbina, why, do, you, even, beef, Sa...   \n",
       "\n",
       "                                         text_tok_keras  \\\n",
       "490   [208, 1187, 1424, 14, 209, 22, 282, 33, 2509, ...   \n",
       "1875  [10, 757, 28, 18, 340, 51, 480, 1425, 1426, 0,...   \n",
       "1146  [1427, 517, 518, 2512, 12, 27, 1428, 110, 3, 3...   \n",
       "2878  [4991, 4992, 25, 2513, 0, 0, 0, 0, 0, 0, 0, 0,...   \n",
       "432   [4993, 78, 50, 8, 114, 1813, 4994, 8, 138, 1, ...   \n",
       "\n",
       "                                   text_tok_keras_words  \n",
       "490   [take, public, opinion, on, revenge, with, pak...  \n",
       "1875  [my, goals, are, so, big, they, scare, small, ...  \n",
       "1146  [panicatthedisco, hey, y'all, announced, it, l...  \n",
       "2878  [christy_rtr, doge_e_fresh, i'm, despondent, ,...  \n",
       "432   [sarah_urbina, why, do, you, even, beef, sara,...  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#map back to words\n",
    "reverse_word_map = dict(map(reversed, tokenizer_keras.word_index.items()))\n",
    "train_df['text_tok_keras_words'] = train_df['text_tok_keras'].apply(lambda x_list: [reverse_word_map[x] if x>0 else \"\" for x in x_list])\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we use the Tensorflow Hub to charge a pretrained Elmo model. TensorFlow Hub is a library for reusable machine learning models. You can learn more here:\n",
    "Source: (https://www.tensorflow.org/hub)\n",
    "Make sure tensor has appropriate size!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Saver not created because there are no variables in the graph to restore\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "#load elmo\n",
    "elmo = hub.Module(\"https://tfhub.dev/google/elmo/3\", trainable = True)\n",
    "\n",
    "#generic way to generate an array of the same length\n",
    "token_len = np.empty(len(train_df))\n",
    "token_len.fill(maxlen)\n",
    "\n",
    "#create embeddings\n",
    "embeddings = elmo(inputs={\"tokens\": list(train_df['text_tok_keras_words']),\n",
    "                          \"sequence_len\": token_len},\n",
    "                  signature=\"tokens\",\n",
    "                  as_dict=True)[\"elmo\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([Dimension(3613), Dimension(30), Dimension(1024)])"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#check the Embedding layer dimension\n",
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To embed a word, you need to pass the position of the token. Let's take the first sentence:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@SusannahSpot I could pop round '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['text'].iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this might take a long time, make sure you can run Tf on your computer\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session()\n",
    "sess.run(init)\n",
    "\n",
    "print(\"SusannahSpot\")\n",
    "print(sess.run(embeddings[0][1]))\n",
    "\n",
    "print(\"I\")\n",
    "print(sess.run(embeddings[0][1]))\n",
    "\n",
    "print(\"could\")\n",
    "print(sess.run(embeddings[0][2]))\n",
    "\n",
    "print(\"pop\")\n",
    "print(sess.run(embeddings[0][3]))\n",
    "\n",
    "print(\"round\")\n",
    "print(sess.run(embeddings[0][4]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "oldHeight": 594.85,
   "position": {
    "height": "40px",
    "left": "723px",
    "right": "20px",
    "top": "80px",
    "width": "250px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "varInspector_section_display": "none",
   "window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
